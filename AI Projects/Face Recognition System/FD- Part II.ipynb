{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912a5b26-7bd7-48a4-8a63-f29de9b43158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable camera ## capture live image ## generate embeeding ## compare the image from dataset ## final outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894bc3fb-91bf-4814-811a-472120372b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\Downloads\\anc\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity            # Import all libraries and modules\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df986f9c-aefd-4331-83a0-413d8a0f4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to enable camera and capture image\n",
    "def capture_image():\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\")  # for face detection\n",
    "    video_cap = cv2.VideoCapture(0)                                                                  # enable the camera  # run time enable\n",
    "    while(1):                                                                                        # camera on permanently, condition always true ,infinite loop\n",
    "        ret, video = video_cap.read()                                                                # read the image\n",
    "        color = cv2.cvtColor(video, cv2.COLOR_BGR2GRAY)                                              # convert the image in BGR channel\n",
    "        faces = face_cascade.detectMultiScale(color, scaleFactor = 1.1,minNeighbors = 5, minSize = (50, 50),flags = cv2.CASCADE_SCALE_IMAGE)  # (color, reduced image size at each scale (1.1 - 10%), minmimum size of the face( 30 refers to minimum width * 30 refers to minimum height),Fine tuning)  \n",
    "    \n",
    "        for(x,y,w,h) in faces:                                                                       # ( w - width of rectangle,  h - height of rectangle)\n",
    "            cv2.rectangle(video, (x,y), (x+w, y+h),(0, 255,0),2)                                     # (image, cordinate of top left corner, coordinates of bottom right corner, BGR format color, thickness of rectangle)\n",
    "        cv2.imshow(\"video_live\", video)                                                              # to create a frame\n",
    "        if cv2.waitKey(5) == ord(\"s\"):                                                               # stop by pressing 's'\n",
    "            image_path = os.path.join(r\"C:\\Users\\hp\\Desktop\\live data\", \"live1.jpg\")                 # provide path to the image\n",
    "            cv2.imwrite(image_path, video)\n",
    "            video_cap.release()\n",
    "            return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6ef49e-5b8d-4360-b767-ced8296618d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check blurrness\n",
    "\n",
    "def check_blurr(image_path, threshold = 100):           # creat a function to check the image\n",
    "    image = cv2.imread(image_path)\n",
    "    lap = cv2.Laplacian(image, cv2.CV_64F)              # compute the laplacian of an image\n",
    "    var = lap.var()                                     # store the variance into var variable\n",
    "\n",
    "    return var < threshold, var                         # return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbcfa569-d7ac-45d0-8d6f-9c52aac83213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b46d45-c943-4e05-946a-e1a6df86150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_attendance(image_names, similarity):\n",
    "        attendance_record = {\n",
    "        'Date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'Image Name': image_names,\n",
    "        'Similarity Score': similarity,\n",
    "        'Attendance Status': 'Present' #if similarity >= threshold else 'Absent'\n",
    "    }\n",
    "        Record = r\"C:\\Users\\hp\\Desktop\\Attendance_Record.xlsx\"\n",
    "        current = pd.read_excel(r\"C:\\Users\\hp\\Desktop\\Attendance_Record.xlsx\")               # function to mark the attendance\n",
    "        attendance_df = pd.DataFrame([attendance_record])\n",
    "        updated_df = pd.concat([current, attendance_df])\n",
    "    \n",
    "        updated_df.to_excel(r\"C:\\Users\\hp\\Desktop\\Attendance_Record.xlsx\")\n",
    "        print(\"Attendance marked successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f83dda3-11dc-4f01-adec-f7c34feeee40",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\deriv.cpp:792: error: (-215:Assertion failed) !_src.empty() in function 'cv::Laplacian'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m capture_image()                                                                         \u001b[38;5;66;03m# capture the image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m blurry, variance \u001b[38;5;241m=\u001b[39m check_blurr(image_path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blurry:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage is clear and not blurry!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcheck_blurr\u001b[1;34m(image_path, threshold)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_blurr\u001b[39m(image_path, threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):           \u001b[38;5;66;03m# creat a function to check the image\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m----> 5\u001b[0m     lap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mLaplacian(image, cv2\u001b[38;5;241m.\u001b[39mCV_64F)              \u001b[38;5;66;03m# compute the laplacian of an image\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     var \u001b[38;5;241m=\u001b[39m lap\u001b[38;5;241m.\u001b[39mvar()                                     \u001b[38;5;66;03m# store the variance into var variable\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m var \u001b[38;5;241m<\u001b[39m threshold, var\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\deriv.cpp:792: error: (-215:Assertion failed) !_src.empty() in function 'cv::Laplacian'\n"
     ]
    }
   ],
   "source": [
    "image_path = capture_image()                                                                         # capture the image\n",
    "blurry, variance = check_blurr(image_path)\n",
    "\n",
    "if not blurry:\n",
    "    print(\"Image is clear and not blurry!!\")\n",
    "    embedding = DeepFace.represent(img_path = image_path, model_name = \"Facenet\")\n",
    "    #live_emb = np.array(embedding)\n",
    "    print(\"Face Embedding: \", embedding[0][\"embedding\"])                 #\n",
    "else:\n",
    "    print(\"Blurr image: will be discarded/ not saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ad7a8-993a-461b-a2b2-340a1295e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_emb = np.array(embedding[0][\"embedding\"])\n",
    "Live_E = live_emb.reshape(1, -1)\n",
    "print(Live_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3658f5-1d02-406b-8366-30e92f7d9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Live_E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f37a60-0dfc-487e-b92d-fa406a696680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client\n",
    "\n",
    "client = Client('localhost')                                     # enable and connect database \n",
    "Result = \"SELECT embedding FROM Data.Embeddings\"                 # extact embeddings from the database and store them into results \n",
    "stored_data = client.execute(Result)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa7ceb-6ab5-46dd-8991-45eab8df6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Embed = np.array(stored_data)                              # convert the data into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5869b-6e9c-4ca4-8d86-e8f60b51573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_Emb = final_Embed.reshape(7, -1)                               # reshaping the data for seperate entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14a56f-e40e-4deb-83ad-1e50d1bed069",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F_Emb)                                                     # print to check whether done correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31efe0f-8b04-4103-9ae8-b091edc7d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F_Emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e957f-d429-423e-82fd-55486b4dc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "image_names = [\"1.jpg\", \"2.jpg\", \"3.jpg\", \"4.jpg\", \"5.jpg\", \"6.jpg\"]\n",
    "\n",
    "#for embedding in F_Emb:\n",
    "similarity = cosine_similarity(Live_E, F_Emb)                                      # check similarity for best match\n",
    "similarities = similarity.flatten()                                                # use flatten to convert 2d array to 1d array\n",
    "print(similarities)                                                                # print all the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e46563-0f95-4459-b937-4bd645350569",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, similiarity in enumerate(similarities):\n",
    "    if (similiarity>= threshold):\n",
    "        print(f\"Live capture matched with : {image_names[i]}, Similarity: {similiarity}\")         # print the best matched image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c8513-3485-4f9e-bc7b-7a1826332b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mark_attendance(image_names, similarity)                                          # mark the attandance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe494a-0532-40ee-b01a-2e59d4404623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee1efc-9d6d-40b2-a916-8b0a7d5b3ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
